{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Federation with Apache Spark\n",
    "In this notebook we'll explore how Spark's Structured Streaming API makes it easy to do data federation between\n",
    "heterogeneous data sources, data at rest and streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Spark with JDBC and Kafka\n",
    "In order to work with Kafka and Spark's JDBC API, you'll need to provide Spark with some additional java packages. Many of these are available as Maven-style packages, such as the Kafka driver and the driver for postgresql in the cell below.\n",
    "\n",
    "Consuming JDBC drivers via Maven coordinates and spark.jars.packages is convenient, since Spark will automatically download such packages and install them on Spark executors.\n",
    "\n",
    "Spark can receive its configuration parameters from a variety of channels. In general, configurations set via a SparkConf object (as below) will override all other configurations. However, there are a few glitches in this rule, and the spark.package.jars parameter is one of them, which is important for this tutorial. To maximize clarity, this notebook unsets PYSPARK_SUBMIT_ARGS in favor of doing all configurations using a SparkConf object so that it is easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Disable this so that configuration of 'spark.jars.packages' works correctly\n",
    "if 'PYSPARK_SUBMIT_ARGS' in os.environ:\n",
    "    del os.environ['PYSPARK_SUBMIT_ARGS']\n",
    "\n",
    "# Instantiate a spark configuration object to receive settings\n",
    "spark_conf = SparkConf()\n",
    "\n",
    "# Maven coordinates for package containing JDBC drivers\n",
    "jdbc_driver_packages = 'org.postgresql:postgresql:42.2.9,org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.3'\n",
    "\n",
    "# Configure spark to see the postgresql driver package\n",
    "spark_conf.set('spark.jars.packages', jdbc_driver_packages)\n",
    "\n",
    "# The name of your Spark cluster hostname or ip address\n",
    "spark_cluster = os.environ['SPARK_CLUSTER']\n",
    "\n",
    "# Configure some basic spark cluster sizing parameters\n",
    "spark_conf.set('spark.cores.max', 2)\n",
    "spark_conf.set('spark.executor.cores', '1')\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('spark://{cluster}:7077'.format(cluster=spark_cluster)) \\\n",
    "    .appName('Spark-Demo') \\\n",
    "    .config(conf = spark_conf) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking driver configurations\n",
    "When you are configuring Spark with extra drivers, it can be useful to sanity check that you have installed what you thought you did. The next cells use `getConf()` to sanity-check Spark's jar file configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'postgresql' in spark.sparkContext.getConf().get('spark.jars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'kafka' in spark.sparkContext.getConf().get('spark.jars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Data Frames\n",
    "Spark's preferred data model is the DataFrame. In the following cell, we are going to load some simulated\n",
    "credit card fraud data from a raw CSV file, into a python pandas DataFrame, and then tell Spark to create its own Spark Data Frame from pandas.\n",
    "\n",
    "You can see that this data includes a numeric user id and a similar merchant id.\n",
    "In the next cells, we'll be setting up a DataFrame join operation to replace these id numbers with names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+------+-----------+-----------+-------+\n",
      "| timestamp|     label|user_id|amount|merchant_id| trans_type|foreign|\n",
      "+----------+----------+-------+------+-----------+-----------+-------+\n",
      "|1591150620|legitimate|      8|  9.42|          4|     online|  false|\n",
      "|1591150669|legitimate|      9| 36.38|         11|contactless|  false|\n",
      "|1591152891|legitimate|      7| 10.56|          8|     online|  false|\n",
      "|1591157902|legitimate|      9| 25.09|         11|      swipe|  false|\n",
      "|1591157988|legitimate|      8| 17.73|          4|     manual|  false|\n",
      "+----------+----------+-------+------+-----------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "transactionDF = spark.createDataFrame(pd.read_csv(\"fraud.csv\"))\n",
    "transactionDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring some new Data Frame tables\n",
    "In the following cell, we are creating a new DataFrame that maps user ids into names.\n",
    "For this demo, our name table is small, and we are creating it manually.\n",
    "A real table for an enterprise would reside in something like a SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|user_id|user_name|\n",
      "+-------+---------+\n",
      "|      0|     Jake|\n",
      "|      1|  Sherman|\n",
      "|      2|   Morgan|\n",
      "|      3|    Bodie|\n",
      "|      4|      Ben|\n",
      "+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [\"Jake\", \"Sherman\", \"Morgan\", \"Bodie\", \"Ben\", \"Elwood\", \"Sandy\", \"Clover\", \"Molly\", \"Linsey\"]\n",
    "raw = [(j, names[j]) for j in range(10)]\n",
    "tdf = spark.createDataFrame(raw, ['user_id', 'user_name'])\n",
    "userDF = tdf.select(tdf.user_id.cast(\"int\"), tdf.user_name)\n",
    "userDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we create a similar table for merchant names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|merchant_id| merchant_name|\n",
      "+-----------+--------------+\n",
      "|          0|Changing Hands|\n",
      "|          1|   First Draft|\n",
      "|          2|       Anaya's|\n",
      "|          3|          SBUX|\n",
      "|          4|         Ike's|\n",
      "+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [\"Changing Hands\", \"First Draft\", \"Anaya's\", \"SBUX\", \"Ike's\", \"Bob's Burgers\", \"Lovin Spoonful\", \"Ayse's\", \"Dawn Treader\", \"Taco Hut\", \"Johnny's Hots\", \"Pavement\", \"Victrola\", \"The Barn\", \"Clockwork\"]\n",
    "raw = [(j, names[j]) for j in range(15)]\n",
    "tdf = spark.createDataFrame(raw, ['merchant_id', 'merchant_name'])\n",
    "merchantDF = tdf.select(tdf.merchant_id.cast(\"int\"), tdf.merchant_name)\n",
    "merchantDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering DataFrames as Tables\n",
    "Spark allows you to register DataFrames with table names that can be used with SQL queries.\n",
    "Hre we'll register our tables so that we can operate on them with SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionDF.registerTempTable(\"transactions\")\n",
    "userDF.registerTempTable(\"users\")\n",
    "merchantDF.registerTempTable(\"merchants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Joins with Spark Data Frames\n",
    "Spark Data Frames support all of the join operations supported in common SQL dialects,\n",
    "and you can operate on them with SQL queries.\n",
    "In this cell, we are setting up a join with our user and merchant tables to replace id numbers with names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+-------+------------+------+\n",
      "| timestamp|user_name| merchant_name|foreign|  trans_type|amount|\n",
      "+----------+---------+--------------+-------+------------+------+\n",
      "|1601491845|     Jake|Changing Hands|  false|chip_and_pin| 16.37|\n",
      "|1593029512|     Jake|Changing Hands|  false|chip_and_pin|  16.2|\n",
      "|1595585679|     Jake|Changing Hands|  false|      manual| 15.51|\n",
      "|1596822054|     Jake|Changing Hands|  false| contactless| 15.15|\n",
      "|1603014673|   Linsey|Changing Hands|  false|chip_and_pin| 21.41|\n",
      "+----------+---------+--------------+-------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinq = \"\"\"\n",
    "SELECT timestamp, user_name, merchant_name, foreign, trans_type, amount FROM\n",
    "transactions\n",
    "left join users on transactions.user_id = users.user_id\n",
    "left join merchants on transactions.merchant_id = merchants.merchant_id\n",
    "\"\"\"\n",
    "\n",
    "joinDF = spark.sql(joinq)\n",
    "joinDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped Aggregations with Data Frames\n",
    "Spark supports the common data analysis operations such as aggregating data by the values of a given column.\n",
    "In the following cell we are grouping transactions by user name, and finding the average transaction amount\n",
    "for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|user_name|               avg|\n",
      "+---------+------------------+\n",
      "|   Linsey|42.803226397800174|\n",
      "|    Sandy|40.503693820224726|\n",
      "|   Morgan| 39.76930656934308|\n",
      "|    Molly| 36.44709313264344|\n",
      "|   Elwood| 35.53930167597763|\n",
      "|    Bodie|34.177011217948724|\n",
      "|   Clover|  33.9838043478261|\n",
      "|  Sherman| 33.63665760869564|\n",
      "|     Jake| 30.17052736982641|\n",
      "|      Ben|26.919920948616625|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "g = joinDF.groupBy(\"user_name\").agg(F.mean(\"amount\").alias(\"avg\"))\n",
    "g.sort(F.desc(\"avg\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous aggregation, here we find the average transaction amount for each merchant name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "| merchant_name|               avg|\n",
      "+--------------+------------------+\n",
      "|        Ayse's|52.653333333333336|\n",
      "| Bob's Burgers|  47.9674193548387|\n",
      "|      Victrola|44.008076923076935|\n",
      "|          SBUX|42.444074074074074|\n",
      "|      Pavement| 38.92485021398004|\n",
      "|     Clockwork|38.505380821917676|\n",
      "|Lovin Spoonful| 34.72461977186304|\n",
      "|         Ike's|34.193778741865486|\n",
      "|  Dawn Treader|31.942931896883408|\n",
      "| Johnny's Hots| 30.46848484848485|\n",
      "|      The Barn| 27.20685714285713|\n",
      "|      Taco Hut| 26.23961538461538|\n",
      "|   First Draft| 23.74031250000001|\n",
      "|Changing Hands|23.066451612903233|\n",
      "|       Anaya's|22.162608695652178|\n",
      "+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = joinDF.groupBy(\"merchant_name\").agg(F.mean(\"amount\").alias(\"avg\"))\n",
    "g.sort(F.desc(\"avg\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Data Frames with a SQL Database\n",
    "\n",
    "In the previous section, we worked with Data Frames based on flat files such as CSV.\n",
    "In the next section we'll see that Spark's Structured Streaming allows you to operate on data residing in a SQL Database without changing your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to a SQL Database with JDBC\n",
    "The next cell sets up the parameters Spark uses to connect to an SQL database using JDBC.\n",
    "\n",
    "For a more detailed discussion of Spark and JDBC drivers, see the Red Hat blog\n",
    "[Data integration in the hybrid cloud with Apache Spark and Open Data Hub](https://next.redhat.com/2020/06/16/data-integration-in-the-hybrid-cloud-with-apache-spark-and-open-data-hub/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_jdbc_url = 'jdbc:postgresql://{host}:{port}/{database}'.format( \\\n",
    "    host     = 'postgresql', \\\n",
    "    port     = '5432', \\\n",
    "    database = 'demo')\n",
    "\n",
    "spark_jdbc_prop = { \\\n",
    "    'user':     'demo', \\\n",
    "    'password': 'demo', \\\n",
    "    'driver':   'org.postgresql.Driver'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SQL Table\n",
    "To set up our example, we will write our transaction data to a SQL database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionDF.write.jdbc(table='transactions', mode='overwrite', url=spark_jdbc_url, properties=spark_jdbc_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Data Frames from JDBC\n",
    "Here we attach a Spark DataFrame to our transaction data stored in Postgresql,\n",
    "and confirm that it is the same as the data we loaded from CSV earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+------+-----------+------------+-------+\n",
      "| timestamp|     label|user_id|amount|merchant_id|  trans_type|foreign|\n",
      "+----------+----------+-------+------+-----------+------------+-------+\n",
      "|1598608959|legitimate|      8| 18.76|          4|      online|  false|\n",
      "|1598609843|legitimate|      3| 129.7|          8|      online|   true|\n",
      "|1598610775|legitimate|      9| 42.84|         14|       swipe|  false|\n",
      "|1598610872|legitimate|      1| 28.08|          6|chip_and_pin|  false|\n",
      "|1598610890|legitimate|      2| 10.29|         11|      online|  false|\n",
      "+----------+----------+-------+------+-----------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactionSQLDF = spark.read.jdbc( \\\n",
    "    table      = '({q}) tmp'.format(q='select * from transactions'), \\\n",
    "    url        = spark_jdbc_url, \\\n",
    "    properties = spark_jdbc_prop \\\n",
    ")\n",
    "\n",
    "transactionSQLDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Joins from JDBC\n",
    "In the following cells, we join our id numbers to names as we did earlier.\n",
    "You can see that our SQL query is exactly the same, excepting the table names that tell spark to\n",
    "use the tables we set up over JDBC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+-------+------------+------+\n",
      "| timestamp|user_name| merchant_name|foreign|  trans_type|amount|\n",
      "+----------+---------+--------------+-------+------------+------+\n",
      "|1601491845|     Jake|Changing Hands|  false|chip_and_pin| 16.37|\n",
      "|1593029512|     Jake|Changing Hands|  false|chip_and_pin|  16.2|\n",
      "|1595585679|     Jake|Changing Hands|  false|      manual| 15.51|\n",
      "|1596822054|     Jake|Changing Hands|  false| contactless| 15.15|\n",
      "|1592832148|   Clover|Changing Hands|  false| contactless| 20.61|\n",
      "+----------+---------+--------------+-------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactionSQLDF.registerTempTable(\"transactionsSQL\")\n",
    "joinq = \"\"\"\n",
    "SELECT timestamp, user_name, merchant_name, foreign, trans_type, amount FROM\n",
    "transactionsSQL\n",
    "left join users on transactionsSQL.user_id = users.user_id\n",
    "left join merchants on transactionsSQL.merchant_id = merchants.merchant_id\n",
    "\"\"\"\n",
    "joinSQLDF = spark.sql(joinq)\n",
    "joinSQLDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|user_name|               avg|\n",
      "+---------+------------------+\n",
      "|   Linsey| 42.80322639780018|\n",
      "|    Sandy|40.503693820224726|\n",
      "|   Morgan|39.769306569343094|\n",
      "|    Molly| 36.44709313264344|\n",
      "|   Elwood| 35.53930167597763|\n",
      "|    Bodie|  34.1770112179487|\n",
      "|   Clover|  33.9838043478261|\n",
      "|  Sherman|33.636657608695636|\n",
      "|     Jake| 30.17052736982641|\n",
      "|      Ben|26.919920948616596|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = joinSQLDF.groupBy(\"user_name\").agg(F.mean(\"amount\").alias(\"avg\"))\n",
    "g.sort(F.desc(\"avg\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "| merchant_name|               avg|\n",
      "+--------------+------------------+\n",
      "|        Ayse's|52.653333333333336|\n",
      "| Bob's Burgers|47.967419354838704|\n",
      "|      Victrola|44.008076923076935|\n",
      "|          SBUX| 42.44407407407408|\n",
      "|      Pavement| 38.92485021398003|\n",
      "|     Clockwork|38.505380821917754|\n",
      "|Lovin Spoonful|34.724619771863104|\n",
      "|         Ike's|34.193778741865415|\n",
      "|  Dawn Treader|31.942931896883348|\n",
      "| Johnny's Hots| 30.46848484848485|\n",
      "|      The Barn|27.206857142857135|\n",
      "|      Taco Hut| 26.23961538461538|\n",
      "|   First Draft|23.740312500000005|\n",
      "|Changing Hands|23.066451612903226|\n",
      "|       Anaya's|22.162608695652178|\n",
      "+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = joinSQLDF.groupBy(\"merchant_name\").agg(F.mean(\"amount\").alias(\"avg\"))\n",
    "g.sort(F.desc(\"avg\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark and Data Frames with Streaming Data\n",
    "We have seen that Spark Data Frames operate the same whether they are based on file format such as CSV or SQL databases such as Postgresql.\n",
    "With Spark Data Frames and Structured Streaming we can also operate on streaming data.\n",
    "The following section we perform the same table joins and grouped aggregations on streaming data pulled off of a Kafka topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attaching a DataFrame to Kafka streaming data\n",
    "The following cell shows an example of how to attach a Spark Data Frame to streaming data - in this case, data pulled off of a Kafka topic.\n",
    "\n",
    "In our example, the data records have been encoded as JSON objects, and so we have added some additional `select` statements to unpack the JSON and impose a Data Frame schema that corresponds to our examples above.\n",
    "\n",
    "To populate this kafka topic, you can run the companion notebook `kafka-product.ipynb` in this git repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionsStreamDF = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"odh-message-bus-kafka-bootstrap:9092\") \\\n",
    "    .option(\"subscribe\", \"demotopic\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load() \\\n",
    "    .select(F.from_json(F.col(\"value\").cast(\"string\"), transactionDF.schema).alias(\"json\")) \\\n",
    "    .select(F.col(\"json.timestamp\"),F.col(\"json.user_id\"),F.col(\"json.merchant_id\"),F.col(\"json.trans_type\"),F.col(\"json.foreign\"),F.col(\"json.amount\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL with Streaming Data is Same as Data at Rest\n",
    "In the following cell we can see that, once again, our SQL for joining with user and merchant tables is exactly the same as our previous examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionsStreamDF.registerTempTable(\"transactionsStream\")\n",
    "joinq = \"\"\"\n",
    "SELECT timestamp, user_name, merchant_name, foreign, trans_type, amount FROM\n",
    "transactionsStream\n",
    "left join users on transactionsStream.user_id = users.user_id\n",
    "left join merchants on transactionsStream.merchant_id = merchants.merchant_id\n",
    "\"\"\"\n",
    "joinStreamDF = spark.sql(joinq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Operations\n",
    "True streaming queries, such as the ones below, must be aggregations,\n",
    "so in this final streaming example we move directly to our data aggregations.\n",
    "\n",
    "Note that in this variation, we attach a `queryName` to each result, so that we can poll the accumulating results as the data comes off the Kafka stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamquery_user = joinStreamDF.groupBy(\"user_name\").agg(F.mean(\"amount\").alias(\"avg\")).sort(F.desc(\"avg\")).writeStream \\\n",
    "    .trigger(processingTime='1 seconds') \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"stream_results_user\") \\\n",
    "    .start()\n",
    "\n",
    "streamquery_merchant = joinStreamDF.groupBy(\"merchant_name\").agg(F.mean(\"amount\").alias(\"avg\")).sort(F.desc(\"avg\")).writeStream \\\n",
    "    .trigger(processingTime='1 seconds') \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"stream_results_merchant\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polling Query Results from Streaming Data\n",
    "A defining characteristic of streaming data queries in Spark is that they are updated continuously as data comes in off the stream.\n",
    "This cell accesses the temporary tables we set up above with `queryName` to display the latest results.\n",
    "You can re-run this cell and watch the results update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|user_name|               avg|\n",
      "+---------+------------------+\n",
      "|   Linsey|          67.50225|\n",
      "|    Molly|          37.81825|\n",
      "|    Bodie| 35.84312500000001|\n",
      "|     Jake| 34.74527272727272|\n",
      "|      Ben|30.797297297297302|\n",
      "|   Morgan|29.498249999999995|\n",
      "|  Sherman|27.860357142857147|\n",
      "|   Elwood|23.871724137931032|\n",
      "|   Clover|22.583666666666673|\n",
      "|    Sandy| 22.44607142857143|\n",
      "+---------+------------------+\n",
      "\n",
      "+--------------+------------------+\n",
      "| merchant_name|               avg|\n",
      "+--------------+------------------+\n",
      "|         Ike's|45.988131868131866|\n",
      "|      Pavement|            39.878|\n",
      "|     Clockwork|32.274155844155835|\n",
      "|  Dawn Treader| 31.57601769911505|\n",
      "|Lovin Spoonful| 30.97042253521127|\n",
      "|   First Draft|             23.17|\n",
      "|       Anaya's|             20.81|\n",
      "| Bob's Burgers|             18.85|\n",
      "+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"stream_results_user\").show()\n",
    "spark.table(\"stream_results_merchant\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Halting a Streaming Query\n",
    "To halt the streaming queries, execute this cell.\n",
    "Otherwise, they will run until the notebook kernel is halted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamquery_user.stop()\n",
    "streamquery_merchant.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
